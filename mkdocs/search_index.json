{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to FluidFS\n\n\n\n\nFluidFS is a personal, wide-area, distributed file system that provides better responsiveness and stronger consistency guarantees than prior systems. It supports multiple, sometimes-collaborating users each with multiple personal devices and computing nodes. Users are geographically distributed, potentially across continents and as a result FluidFS must be responsive to network conditions. To provide responsive strong consistency, FluidFS implements a novel form of distributed coordination called \nhierarchical consensus\n, currently being researched at the University of Maryland.\n\n\nThe goals of FluidFS are a system with the following characteristics:\n\n\n\n\nfault tolerance\n: robust to device failures and network partitions\n\n\nflexibility\n: able to add and remove devices through local subquorums\n\n\navailability\n: mobile users can access data when centralized storage is either unavailable or too slow\n\n\nconcurrency\n: simultaneous access by multiple users enables both collaboration and use by users or projects that are unrelated\n\n\ncorrectness\n: data must be consistent to the limits of the storage model\n\n\ndurability\n: data must be durable on the level of years, implying a platform-agnostic view of storage\n\n\n\n\nDecentralized consensus algorithms are used by distributed systems to make global decisions that must be durable. FluidFS uses a variant of the \nRaft consensus algorithm\n called \nhierarchical consensus\n to ensure small quorum sizes, high availability, and strong coordination across geographically distributed devices. Hierarchical consensus flexibly allocates subquorums to dynamic object groupings. This allows availability and performance due to the localization of quorum membership near to where accesses are occurring. It also allows larger quorums because the decision space is segmented across a large number of devices.\n\n\nDevelopment\n\n\nThe primary interface is a command line program that interacts directly with the fluid library. Building from source is implemented using the included Makefile, which fetches dependencies and builds locally rather than to the \n$GOPATH\n:\n\n\n$ make\n\n\n\nThere is an RSpec-style test suite that uses \nGinkgo\n and \nGomega\n. These tests can be run with the Makefile:\n\n\n$ make test\n\n\n\nContributing\n\n\nNote that labels in the Github issues are defined in the blog post: \nHow we use labels on GitHub Issues at Mediocre Laboratories\n.\n\n\nThe repository is set up in a typical production/release/development cycle as described in \nA Successful Git Branching Model\n. A typical workflow is as follows:\n\n\n\n\n\n\nSelect a card from the \ndev board\n - preferably one that is \"ready\" then move it to \"in-progress\".\n\n\n\n\n\n\nCreate a branch off of develop called \"feature-[feature name]\", work and commit into that branch.\n\n\n~$ git checkout -b feature-myfeature develop\n\n\n\n\n\n\n\nOnce you are done working (and everything is tested) merge your feature into develop.\n\n\n~$ git checkout develop\n~$ git merge --no-ff feature-myfeature\n~$ git branch -d feature-myfeature\n~$ git push origin develop\n\n\n\n\n\n\n\nRepeat. Releases will be routinely pushed into master via release branches, then deployed to the server.\n\n\n\n\n\n\nAgile Board and Documentation\n\n\nThe development board can be found on Waffle:\n\n\n\n\nhttps://waffle.io/bbengfort/fluidfs\n\n\n\n\nThe documentation can be built and served locally with \nmkdocs\n:\n\n\n$ mkdocs serve\n\n\n\nThe latest version of the documentation is hosted with GitHub Pages and can be found at the project link:\n\n\n\n\nhttps://bbengfort.github.io/fluidfs\n\n\n\n\nTo build and publish the documentation, use the make file:\n\n\n$ make publish\n\n\n\nThis will use the \nmkdocs gh-deploy\n command to build the site to the gh-pages branch and will push to origin.",
            "title": "Introduction"
        },
        {
            "location": "/#welcome-to-fluidfs",
            "text": "FluidFS is a personal, wide-area, distributed file system that provides better responsiveness and stronger consistency guarantees than prior systems. It supports multiple, sometimes-collaborating users each with multiple personal devices and computing nodes. Users are geographically distributed, potentially across continents and as a result FluidFS must be responsive to network conditions. To provide responsive strong consistency, FluidFS implements a novel form of distributed coordination called  hierarchical consensus , currently being researched at the University of Maryland.  The goals of FluidFS are a system with the following characteristics:   fault tolerance : robust to device failures and network partitions  flexibility : able to add and remove devices through local subquorums  availability : mobile users can access data when centralized storage is either unavailable or too slow  concurrency : simultaneous access by multiple users enables both collaboration and use by users or projects that are unrelated  correctness : data must be consistent to the limits of the storage model  durability : data must be durable on the level of years, implying a platform-agnostic view of storage   Decentralized consensus algorithms are used by distributed systems to make global decisions that must be durable. FluidFS uses a variant of the  Raft consensus algorithm  called  hierarchical consensus  to ensure small quorum sizes, high availability, and strong coordination across geographically distributed devices. Hierarchical consensus flexibly allocates subquorums to dynamic object groupings. This allows availability and performance due to the localization of quorum membership near to where accesses are occurring. It also allows larger quorums because the decision space is segmented across a large number of devices.",
            "title": "Welcome to FluidFS"
        },
        {
            "location": "/#development",
            "text": "The primary interface is a command line program that interacts directly with the fluid library. Building from source is implemented using the included Makefile, which fetches dependencies and builds locally rather than to the  $GOPATH :  $ make  There is an RSpec-style test suite that uses  Ginkgo  and  Gomega . These tests can be run with the Makefile:  $ make test",
            "title": "Development"
        },
        {
            "location": "/#contributing",
            "text": "Note that labels in the Github issues are defined in the blog post:  How we use labels on GitHub Issues at Mediocre Laboratories .  The repository is set up in a typical production/release/development cycle as described in  A Successful Git Branching Model . A typical workflow is as follows:    Select a card from the  dev board  - preferably one that is \"ready\" then move it to \"in-progress\".    Create a branch off of develop called \"feature-[feature name]\", work and commit into that branch.  ~$ git checkout -b feature-myfeature develop    Once you are done working (and everything is tested) merge your feature into develop.  ~$ git checkout develop\n~$ git merge --no-ff feature-myfeature\n~$ git branch -d feature-myfeature\n~$ git push origin develop    Repeat. Releases will be routinely pushed into master via release branches, then deployed to the server.",
            "title": "Contributing"
        },
        {
            "location": "/#agile-board-and-documentation",
            "text": "The development board can be found on Waffle:   https://waffle.io/bbengfort/fluidfs   The documentation can be built and served locally with  mkdocs :  $ mkdocs serve  The latest version of the documentation is hosted with GitHub Pages and can be found at the project link:   https://bbengfort.github.io/fluidfs   To build and publish the documentation, use the make file:  $ make publish  This will use the  mkdocs gh-deploy  command to build the site to the gh-pages branch and will push to origin.",
            "title": "Agile Board and Documentation"
        },
        {
            "location": "/architecture/",
            "text": "Architecture\n\n\n\n\nReplication\n\n\nFluidFS will have three modes of operation with two primary protocols. The consistency protocols include:\n\n\n\n\nBasic Raft\n\n\nHierarchical Consensus\n\n\n\n\nAnd the modes of operation are currently:\n\n\n\n\nBest Effort\n\n\nTransactions\n\n\nLinearizeability\n\n\n\n\nThe prototype version of FluidFS will implement the Basic Raft/Linearizeability consistency protocol and operation mode. This will allow us to set a performance baseline and to build many of the components required for a distributed file system that is coordinated by consensus.\n\n\nThis mode of operation will be implemented through \nlocking\n decisions that are determined by consensus. Locks will eventually be replaced by \nleases\n, which for now we will describe as time limited locks that can be automatically renewed through a heartbeat mechanism.\n\n\nLogs and Caches\n\n\nEach replica will maintain a local \ncache\n of version information for fast lookups. This will be implemented as a key-value store where the namespace of the filesystem points to the latest local version of each entity. The replica can retrieve the version meta data by:\n\n\n\n\nLooking up the latest version for the entity in the cache\n\n\nLooking up the most recent commit for the entity in the cache\n\n\nRequesting the version for the entity from the leader\n\n\n\n\nIn our current mode of operation, a read access will need to lock the file (unless we have read-only accesses) and detect if the file is locked or not.  \n\n\nConsensus decisions order a log - and all replicas will contain an in-memory log of operations. The log will periodically be snapshotted to disk in order to free memory (we can perform a similar mechanism for the cache). There are open questions about how the cache and the log interact.\n\n\nVersion Replication\n\n\nMetadata concerning file system entities are the only objects that need replication in a consistent fashion. When consensus algorithms are used for consistency, they create a single, ordered log that will contain (for our current mode of operation):\n\n\n\n\nLock grants\n\n\nVersion meta data (I propose the log only contain version numbers)\n\n\nLock releases\n\n\n\n\nOn a read access (open):\n\n\n\n\nRead the latest version from the DB (check if locked, if so abort)\n\n\nRequest a lock from the leader (consensus occurs)\n\n\nOn lock commit, update metadata and return the file's blobs.\n\n\n\n\nOn a write access (close):\n\n\n\n\nstore the blobs and the write metadata into the cache (read your writes)\n\n\nforward the remote write to the leader\n\n\nif leader is unavailable, keep retrying\n\n\nboth version decision and lock release occur simultaneously\n\n\nif leader rejects the write (e.g. the lock was removed before the write), mark the conflict on the file (similar to the Git method of marking merge conflicts).\n\n\non commit, update the local metadata\n\n\n\n\nBlob Replication\n\n\nSmarter blob replication will be required for the future but for now:\n\n\n\n\nblobs are stored either in BoltDB or prefixed directories on disk\n\n\nwhen blobs are written, they are replicated via anti-entropy\n\n\nanti-entropy sessions broadcast a list of prefixes and their blob count\n\n\nthe remote returns any additional blobs since that count (pull)\n\n\n\n\nThis will lead to full replication of all blobs, and further inspection of blob replication in the future is required.\n\n\nConfiguration\n\n\nThe system will be configured by YAML files, with increasing priority:\n\n\n\n\n/etc/fluid/fluid.yml\n\n\n$HOME/.fluid/fluid.yml\n\n\n$PWD/fluid.yml\n\n\n\n\nA YAML configuration can be passed in at the command line (to allow us to conduct multiple experiments), however any command line arguments will supersede any configuration in any YAML file.\n\n\nI will also create a configuration server that returns a global configuration via JSON API. If the global_config variable is set, then on startup/reload the system will request the global configuration and \nupdate the /etc/fluid/fluid.yml\n file directly (or possibly create a global.yml file that has a slightly higher priority)!\n\n\nConfiguration will include:\n\n\n\n\nReplica behaviors\n\n\nMount points\n\n\nLogging information\n\n\nDatabase information\n\n\n\n\nThere will also need to be a \nremotes.yml\n file that describes all the possible nodes in the network. This will be similarly updated by the configuration server.\n\n\nThe configuration directory will also require the following:\n\n\n\n\nTLS certificate\n\n\nclient key (though this might be stored in the configuration file)\n\n\n\n\nFS Interface\n\n\nFUSE\n\n\nAPI\n\n\nFSEvents\n\n\nUnderlying Storage\n\n\nWe will use a combination of \nBoltDB\n and the local file system for underlying storage.\n\n\nBoltDB will contain the following buckets:\n\n\n\n\nFS file names to latest version id\n\n\nFS directory names to directory meta data\n\n\nVersion ID to version meta data\n\n\nBlob ID to blob (data or path)\n\n\nBlob state since last anti-entropy\n\n\n\n\nIf we use the file system for blob storage we will split the blob IDs (SHA256 hashes) into subdirectories, such that the first 7 characters of the hash is the first directory, the second, the sub directory, and so on and so forth until the blob data.\n\n\nRPC and Communication\n\n\nCommunication will be conducted by \ngRPC\n and \nProtocol Buffers\n serialization. We will take advantage of gRPC communication patterns, for example some comms will use Unary RPC (single request and response) while others will use the streaming RPC services for many messages per communication.\n\n\nRemoteAccess\n\n\nThis will be a unary rpc from a follower to the leader requesting a lock or a write/release. The leader will return the response immediately to acknowledge it is engaging in consensus decisions.\n\n\nAppendEntries\n\n\nThe Raft AE RPC and heartbeats.\n\n\nElection\n\n\nRPC messages for requesting votes on election timeout.\n\n\nBlob Anti-Entropy\n\n\nRPC messages for conducting pairwise anti-entropy of the blob space.\n\n\nSecurity\n\n\nWe will secure communication over HTTP with:\n\n\n\n\nHAWK authentication for clients\n\n\nTLS for transport layer security\n\n\n\n\nWe also want to have blob encryption, but as this will require key management, we'll reserve this as a stretch goal.\n\n\nDeamonization and Startup\n\n\nI will create LaunchAgent (for OS X) and Upstart scripts so that the FluidFS daemon always runs in the background on startup etc. It will use the mount points configuration to ensure that FUSE is mounted correctly in those directories.",
            "title": "Architecture"
        },
        {
            "location": "/architecture/#architecture",
            "text": "",
            "title": "Architecture"
        },
        {
            "location": "/architecture/#replication",
            "text": "FluidFS will have three modes of operation with two primary protocols. The consistency protocols include:   Basic Raft  Hierarchical Consensus   And the modes of operation are currently:   Best Effort  Transactions  Linearizeability   The prototype version of FluidFS will implement the Basic Raft/Linearizeability consistency protocol and operation mode. This will allow us to set a performance baseline and to build many of the components required for a distributed file system that is coordinated by consensus.  This mode of operation will be implemented through  locking  decisions that are determined by consensus. Locks will eventually be replaced by  leases , which for now we will describe as time limited locks that can be automatically renewed through a heartbeat mechanism.",
            "title": "Replication"
        },
        {
            "location": "/architecture/#logs-and-caches",
            "text": "Each replica will maintain a local  cache  of version information for fast lookups. This will be implemented as a key-value store where the namespace of the filesystem points to the latest local version of each entity. The replica can retrieve the version meta data by:   Looking up the latest version for the entity in the cache  Looking up the most recent commit for the entity in the cache  Requesting the version for the entity from the leader   In our current mode of operation, a read access will need to lock the file (unless we have read-only accesses) and detect if the file is locked or not.    Consensus decisions order a log - and all replicas will contain an in-memory log of operations. The log will periodically be snapshotted to disk in order to free memory (we can perform a similar mechanism for the cache). There are open questions about how the cache and the log interact.",
            "title": "Logs and Caches"
        },
        {
            "location": "/architecture/#version-replication",
            "text": "Metadata concerning file system entities are the only objects that need replication in a consistent fashion. When consensus algorithms are used for consistency, they create a single, ordered log that will contain (for our current mode of operation):   Lock grants  Version meta data (I propose the log only contain version numbers)  Lock releases   On a read access (open):   Read the latest version from the DB (check if locked, if so abort)  Request a lock from the leader (consensus occurs)  On lock commit, update metadata and return the file's blobs.   On a write access (close):   store the blobs and the write metadata into the cache (read your writes)  forward the remote write to the leader  if leader is unavailable, keep retrying  both version decision and lock release occur simultaneously  if leader rejects the write (e.g. the lock was removed before the write), mark the conflict on the file (similar to the Git method of marking merge conflicts).  on commit, update the local metadata",
            "title": "Version Replication"
        },
        {
            "location": "/architecture/#blob-replication",
            "text": "Smarter blob replication will be required for the future but for now:   blobs are stored either in BoltDB or prefixed directories on disk  when blobs are written, they are replicated via anti-entropy  anti-entropy sessions broadcast a list of prefixes and their blob count  the remote returns any additional blobs since that count (pull)   This will lead to full replication of all blobs, and further inspection of blob replication in the future is required.",
            "title": "Blob Replication"
        },
        {
            "location": "/architecture/#configuration",
            "text": "The system will be configured by YAML files, with increasing priority:   /etc/fluid/fluid.yml  $HOME/.fluid/fluid.yml  $PWD/fluid.yml   A YAML configuration can be passed in at the command line (to allow us to conduct multiple experiments), however any command line arguments will supersede any configuration in any YAML file.  I will also create a configuration server that returns a global configuration via JSON API. If the global_config variable is set, then on startup/reload the system will request the global configuration and  update the /etc/fluid/fluid.yml  file directly (or possibly create a global.yml file that has a slightly higher priority)!  Configuration will include:   Replica behaviors  Mount points  Logging information  Database information   There will also need to be a  remotes.yml  file that describes all the possible nodes in the network. This will be similarly updated by the configuration server.  The configuration directory will also require the following:   TLS certificate  client key (though this might be stored in the configuration file)",
            "title": "Configuration"
        },
        {
            "location": "/architecture/#fs-interface",
            "text": "",
            "title": "FS Interface"
        },
        {
            "location": "/architecture/#fuse",
            "text": "",
            "title": "FUSE"
        },
        {
            "location": "/architecture/#api",
            "text": "",
            "title": "API"
        },
        {
            "location": "/architecture/#fsevents",
            "text": "",
            "title": "FSEvents"
        },
        {
            "location": "/architecture/#underlying-storage",
            "text": "We will use a combination of  BoltDB  and the local file system for underlying storage.  BoltDB will contain the following buckets:   FS file names to latest version id  FS directory names to directory meta data  Version ID to version meta data  Blob ID to blob (data or path)  Blob state since last anti-entropy   If we use the file system for blob storage we will split the blob IDs (SHA256 hashes) into subdirectories, such that the first 7 characters of the hash is the first directory, the second, the sub directory, and so on and so forth until the blob data.",
            "title": "Underlying Storage"
        },
        {
            "location": "/architecture/#rpc-and-communication",
            "text": "Communication will be conducted by  gRPC  and  Protocol Buffers  serialization. We will take advantage of gRPC communication patterns, for example some comms will use Unary RPC (single request and response) while others will use the streaming RPC services for many messages per communication.",
            "title": "RPC and Communication"
        },
        {
            "location": "/architecture/#remoteaccess",
            "text": "This will be a unary rpc from a follower to the leader requesting a lock or a write/release. The leader will return the response immediately to acknowledge it is engaging in consensus decisions.",
            "title": "RemoteAccess"
        },
        {
            "location": "/architecture/#appendentries",
            "text": "The Raft AE RPC and heartbeats.",
            "title": "AppendEntries"
        },
        {
            "location": "/architecture/#election",
            "text": "RPC messages for requesting votes on election timeout.",
            "title": "Election"
        },
        {
            "location": "/architecture/#blob-anti-entropy",
            "text": "RPC messages for conducting pairwise anti-entropy of the blob space.",
            "title": "Blob Anti-Entropy"
        },
        {
            "location": "/architecture/#security",
            "text": "We will secure communication over HTTP with:   HAWK authentication for clients  TLS for transport layer security   We also want to have blob encryption, but as this will require key management, we'll reserve this as a stretch goal.",
            "title": "Security"
        },
        {
            "location": "/architecture/#deamonization-and-startup",
            "text": "I will create LaunchAgent (for OS X) and Upstart scripts so that the FluidFS daemon always runs in the background on startup etc. It will use the mount points configuration to ensure that FUSE is mounted correctly in those directories.",
            "title": "Deamonization and Startup"
        },
        {
            "location": "/about/",
            "text": "About\n\n\nIn practice, FluidFS is a research project to create a distributed file system in user space (FUSE) that is highly consistent and reliable. It is meant as a Dropbox replacement, allowing direct synchronization between devices on a personal network rather than going through a cloud service. Strong consistency is provided through a novel approach to distributed coordination called \nhierarchical consensus\n.\n\n\nThis research is being conducted in the Computer Science Department at the University of Maryland. All related research and publications will be posted on this page eventually.",
            "title": "FluidFS"
        },
        {
            "location": "/about/#about",
            "text": "In practice, FluidFS is a research project to create a distributed file system in user space (FUSE) that is highly consistent and reliable. It is meant as a Dropbox replacement, allowing direct synchronization between devices on a personal network rather than going through a cloud service. Strong consistency is provided through a novel approach to distributed coordination called  hierarchical consensus .  This research is being conducted in the Computer Science Department at the University of Maryland. All related research and publications will be posted on this page eventually.",
            "title": "About"
        },
        {
            "location": "/changelog/",
            "text": "Changelog\n\n\nThe release versions that are  tagged in Github. You can see the tags through the Github web application and download the binary of the version you'd like. Note that for many operating systems you'll have to compile for your local architecture (linux and darwin supported).\n\n\nThe versioning uses a three part version system, \"a.b.c\" - \"a\" represents a major release that may not be backwards compatible. \"b\" is incremented on minor releases that may contain extra features, but are backwards compatible. \"c\" releases are bug fixes or other micro changes that developers should feel free to immediately update to.\n\n\nVersion 0.1\n\n\n\n\ntag\n: \npending\n\n\nrelease\n: Pending\n\n\ncommit\n: \nsee tag\n\n\n\n\nThis is the initial prototype version of the file system, and is still pending development.",
            "title": "Changelog"
        },
        {
            "location": "/changelog/#changelog",
            "text": "The release versions that are  tagged in Github. You can see the tags through the Github web application and download the binary of the version you'd like. Note that for many operating systems you'll have to compile for your local architecture (linux and darwin supported).  The versioning uses a three part version system, \"a.b.c\" - \"a\" represents a major release that may not be backwards compatible. \"b\" is incremented on minor releases that may contain extra features, but are backwards compatible. \"c\" releases are bug fixes or other micro changes that developers should feel free to immediately update to.",
            "title": "Changelog"
        },
        {
            "location": "/changelog/#version-01",
            "text": "tag :  pending  release : Pending  commit :  see tag   This is the initial prototype version of the file system, and is still pending development.",
            "title": "Version 0.1"
        }
    ]
}